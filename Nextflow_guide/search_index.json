[["index.html", "MG Nextflow Guide Chapter 1 Intro 1.1 Running nextflow 1.2 Contents 1.3 Links", " MG Nextflow Guide Matthew R. Gemmell 2025-03-25 Chapter 1 Intro This is a quick and dirty guide on how I use Nextflow. It is primarily for my use but if someone else finds it I hope it will also proof useful. Additionally, this was made with R Bookdown. 1.1 Running nextflow To run nextflow move to the directory with the .nf file you want to run and run: nextflow run main.nf -resume -resume is useful to keep so it does not rerun any processes that have already run. However, it can be useful to remove it if you want to test the workflow from the start. 1.2 Contents The chapters in this bookdown include: Concepts: 1.3 Links Useful links include: nextflow.io: The main website for Nextflow training.nextflow.io: Website with tutorials for Nextflow "],["01-Concepts.html", "Chapter 2 Concepts 2.1 Workflow 2.2 Variables 2.3 Channels, tuples, &amp; lists", " Chapter 2 Concepts 2.1 Workflow Nextflow is used to create workflows. A workflow consists of multiple processes. Each process has input and output. A process will not start until it has all the input it requires. This is important as a process will therefore wait till other processes are finished if the output of other processes are the input for the process. In the nextflow script each process is created in the main script body (or in a separate main.nf file). These processes are then called in the workflow. Example main.nf #!/usr/bin/env nextflow /* *Params */ params.dir = &quot;.&quot; params.input1 = &quot;input.txt&quot; /* * Processes */ process STEP1 { input: path(input_data) output: path(&quot;output1.txt&quot;) } process STEP2 { input: path(input_data) output: path(&quot;output2.txt&quot;) } /* * workflow */ workflow { /// Set input param to channel input1_ch=file(params.input1) /// Process 1 step1(input1_ch) /// Process 2 step2(step1.out) } 2.2 Variables Nextflow Variables are used across a nextflow script. Within the script section of a process block they are denoted by $ (e.g. $sample_id). Variable names cannot start with numbers. Bash variables within a script section must be denoted by a \\ (e.g. \\$var). 2.3 Channels, tuples, &amp; lists Channels, tuples, and lists are objects that contain multiple objects but work in different ways. 2.3.1 Channels Channels are specified and used in the workflow section. A Channel contains a number of values. Each value passes through a process separately, this is carried out via parallelisation. Example: A Channel called integers_ch contains the 10 values 1,2,3,4,5,6,7,8, &amp; 9. A process called multiple_by_10 multiples input by 10. If the Channel integers_ch was used as the input for the process multiple_by_10 the output woudl be a Channel of 10 values containing the values 10,20,30,40,50,60,70,80, &amp; 90. 2.3.2 Tuples &amp; Lists Tuples &amp; Lists are used within process blocks. There are many ways to create and manipulate them within and without process blocks. Confusingly Tuples &amp; Lists are both structured as [value_1,value_2,value_3]. 2.3.2.1 Tuples Tuples contain multiple values with each value assigned as a separate variable in a process. This allows you to input/output data which should be grouped together. The below example shows how to group sample ids with their paired fastq reads. process step1 { input: tuple val(sample_id), path(r1), path(r2) output: tuple val(sample_id), path(&quot;${sample_id}_r1_trimmed.fastq&quot;), path(&quot;${sample_id}_r2_trimmed.fastq&quot;) script: &quot;&quot;&quot; trim -i1 $r1 -i2 $r2 \\ -o1 ${sample_id}_r1_trimmed.fastq -o2 ${sample_id}_r2_trimmed.fastq &quot;&quot;&quot; } This is important as multiple Channels with multiple values are not ordered relative to each other. 2.3.2.2 Lists When a List is used within a script block all the values will be used together with a space () between each value. Example: The Channel r1_fastqs_ch contains the List [\"S1_R1.fastq\",\"S2_R1.fastq\",\"S3_R1.fastq\"] The below truncated nextflow script to run fastqc.... process r1_fastqc { input: path(r1s) output: ..... script: &quot;&quot;&quot; fastqc -o fastqc_output \\ $r1s &quot;&quot;&quot; } workflow { r1_fastqc(r1_fastqs_ch) } Would be run as: fastqc -o fastqc_output \\ S1_R1.fastq S2_R1.fastq S3_R1.fastq 2.3.2.3 Combinations Of course you can have a Channel that can contain multiple Tuples and/or Lists. Additionally Tuples can contain Lists. "],["02-Basic_layout.html", "Chapter 3 Basic layout 3.1 main.nf 3.2 Params 3.3 Processes 3.4 workflow", " Chapter 3 Basic layout When using Nextflow each project should be within its own directory. Within the main directory of the project you will have all the Nextflow files you need. There are many Nextflow files you can have but the only essential file is a main .nf file which many people like to name as main.nf. This chapter will go over a basic main.nf file and its different parts. 3.1 main.nf The main.nf is a plain text file that contains Nextflow code. Below is an example: #!/usr/bin/env nextflow /* *Params */ params.dir = &quot;.&quot; params.outdir = &quot;./results&quot; params.input_file = &quot;/path/to/data/input.txt&quot; /* * Processes */ process STEP1 { publishDir: params.outdir, mode: &#39;copy&#39; input: path(input_data) output: path(&quot;output1.txt&quot;) script: &quot;&quot;&quot; create_output -i $input_data -o output1.txt &quot;&quot;&quot; } process STEP2 { publishDir: params.outdir, mode: &#39;copy&#39; input: path(input_data) output: path(&quot;output2.txt&quot;) script: &quot;&quot;&quot; create_output_2 -i $input_data -o output2.txt &quot;&quot;&quot; } /* * workflow */ workflow { /// Set input params to channels input_ch=file(params.input_file) /// Process 1 STEP1(input_ch) /// Process 2 STEP2(step1.out) } 3.2 Params The first section is the various initial parameters. This is useful for specifying input information for the workflow including: Input &amp; output directories Input data Metadata information Reference files Any time they are used anywhere within the script they must include params. as a prefix. 3.3 Processes Processes are how the tasks of a workflow are specified and have many parts. Process reference 3.3.1 Initialistion A process is defined by process NAME and the process body is cotnained with {}. The Process name is arbitrary and decided by the workflow designer. However some suggestions are: Do not start the name with numbers. Capitalise all letters used, this is normal convention and makes it easier to see what are processes in your workflow block. Separate words with _. Ensure all process names are unique and somewhat descriptive within your workflow. 3.3.2 publishDir All files created by the Process will be contained with the workflow's work directory (more info later). All files specified in the output section will be stored in the directory specified by publishDir, if the directory does not exist Nextflow will create it. The output data is stored in the specified based on the mode:. The modes are: copy: Copies the output files into the publish directory. copyNoFollow: Copies the output files into the publish directory without following symlinks ie. copies the links themselves. link: Creates a hard link in the publish directory for each output file. move: Moves the output files into the publish directory. Note: this is only supposed to be used for a terminal process i.e. a process whose output is not consumed by any other downstream process. rellink: Creates a relative symbolic link in the publish directory for each output file. symlink: Creates an absolute symbolic link in the publish directory for each output file (default). More info on publishDir 3.3.3 input These specify the input names and type. Input names are arbitrary but should follow variables rules. The 2 basic input types are: path: Specifies paths, generally poiting to files. val: Specifies values, these are generally text or numbers such as sample ids. 3.3.4 output These specify the output. If the output is a file it needs to be the actual name of the output file in quotes (double quotes are normally preferred). 3.3.5 script The script block, denoted by flanking triple double quotes (\"\"\"), contains the code the process will carry out. By default this will be bash code. Input variables can be denoted by a $ and output variables are written like normal script but can also use input variables such as sample ids. 3.4 workflow The workflow section is where all the other sections come together so the workflow knows how to utilise them. "],["03-Channels_tuples_lists.html", "Chapter 4 Channels, tuples, &amp; lists 4.1 Channels 4.2 Tuples 4.3 List", " Chapter 4 Channels, tuples, &amp; lists This section shows how to create, edit, and manipulate channels, tuples, and lists. For an explanation on these concepts please see chapter 2 4.1 Channels For more info on channels including channel operators please see the nextflow docs 4.1.1 Creating channels There are many ways to create various types of channels. 4.1.1.1 Text workflow { hello_ch = Channel.of(&quot;Hello&quot;,&quot;Bonjour&quot;) } 4.1.1.2 Data file data_ch = Channel.fromPath(params.input_file) or workflow { data_ch = file(params.input_file) } 4.1.1.3 CSV of files workflow { files_ch = Channel.fromPath(params.input_file) .splitCsv() .flatten() } 4.1.1.4 FOFN A file containing file names sperated by lines workflow { files_ch = Channel.fromPath(params.file).splitText() } 4.1.1.5 Paired files Useful for paired illumina fastq files. params.reads = &quot;/path_to_fastq_dir/*_R{1,2}.fastq&quot; workflow { Channel .fromFilePairs(params.reads, checkIfExists: true) .set {read_pairs_ch} } This creates a channel of multiple values based on the number of paired files. Each channel value has contains a tuple of 2 values: First is a text value of the file prefix. The file prefix is the text that the * reprsents in the params.reads path. The second is a list with the first value is the R1 file and the second value is the R2 file. For example if you had the files: S1_R1.fastq S1_R2.fastq The tuple would be: [\"S1\",[\"S1_R1.fastq\",\"S1_R2.fastq]] You could then use this channel like so: params.reads = &quot;/path_to_fastq_dir/*_R{1,2}.fastq&quot; process RUN { input: tuple val(sample_id), path(reads) script: &quot;&quot;&quot; command -s ${sample_id} -r1 ${reads[0]} -r2 ${reads[1]} &quot;&quot;&quot; } workflow { Channel .fromFilePairs(params.reads, checkIfExists: true) .set {read_pairs_ch} RUN(read_pairs_ch) } Note: Indexing a list starts at 0 (-r1 ${reads[0]}) 4.1.2 Extracting channels from process process RUN { input: path(R1) output: path(&quot;R1_trimmed.fastq&quot;), emit: R1_trimmed path(&quot;R1_trimmed.stats.txt&quot;), emit: R1_trimmed_stats } workflow { /// Input reads from params r1_ch = file(params.r1_fastq) /// Run process RUN(r1_ch) /// Extract channels from run process r1_trimmed_ch = RUN.out.R1_trimmed r1_trimmed_stats_ch = RUN.out.R1_trimmed_stats } Note: You only need to extract channels form a process if they are going to be used in another process. 4.2 Tuples 4.2.1 Input &amp; Output tuples process RUN { input: tuple val(id), path(R1), path(R2) output: tuple val(id), path(“R1_trim.fastq”), file(“R2_trim.fastq”) } 4.2.2 Transpose If you have the below channel of 3 values each containing a tuple of 2.... [[“s1”,”s1.txt”],[“s2”,”s2.txt”],[“s3”,”s3.txt”]] and want to change it to the below channel of 1 value containing a tuple of 2, each tuple containing a list of 3.... [[“s1”,”s2”,”s3”],[“s1.txt”,”s2.txt”,”s3.txt”]] You can run the following workflow { transposed_ch = id_file_pair_ch.collect(flat:false) .map{ it.transpose()} } This can the be called as input to a process like so: process RUN { input: tuple val(sample_ids_list), path(files_list) } 4.3 List 4.3.1 Channel values to one list You can collect all the values of a channel into one list. workflow { all_R1_fastq_ch = FASTQC.out.htmlR.collect() all_R2_fastq_ch = FASTQC.out.htmlL.collect() } This ends up with lists like [“S1_R1.fastqc.html”, “S2_R1.fastqc.html”] 4.3.2 Parameter in front of each list value Some programs accept a list of values separated by spaces. However, some may require a parameter flag in front of each path/file. This can be carried out by setting a definition in the process block. process RUN { input: path(all_R1_fastq) script: def R1_fastqc_lines = all_R1_fastq.collect { fastqc -&gt; “-F ${fastqc}” }.join(‘ ‘) &quot;&quot;&quot; multiqc -o multiqc $R1_fastqc_lines &quot;&quot;&quot; } That then becomes “-F S1_R1.fastqc.html -F S2_R1.fastqc.html” "],["04-Processes.html", "Chapter 5 Processes 5.1 Directives 5.2 Script 5.3 Modules", " Chapter 5 Processes This section includes some extra features related to processes. A full link of process info can be seen in the nextflow docs 5.1 Directives You can include directives in each process. These can be used to specify the execution requirements of a process. Link to full list of process directives 5.1.1 cpus One example is that you can use cpus to specify the number of cpus to be used by a process. process RUN { cpus 2 } 5.1.2 publishDir The basics of pblishDir My preferred method is to assign an overall output/results directory as a params. params.outdir=&quot;./results&quot; process RUN { publishDir params.outdir, mode: &#39;copy&#39; } You can also set a subdiretcory of the output directory in the process. params.outdir=&quot;./results&quot; process RUN { publishDir { &quot;${params.outdir}/stage_1&quot; }, mode: &#39;copy&#39; } You can even do this with input variables. params.outdir=&quot;./results&quot; input: val sample_id process RUN { publishDir { &quot;${params.outdir}/stage_1/${sample_id}&quot; }, mode: &#39;copy&#39; } 5.1.3 conda A conda environment can be specified. Further info on using conda environments 5.1.3.1 Local environment You can specify a conda environment you have locally created. process RUN { conda &quot;/home/minforge3/envs/run_env&quot; } If using a locally installed env it is best to specify it as a params to make it quicker to add.edit for multiple processes. params.conda_env=&quot;/home/minforge3/envs/run_env&quot; process RUN { conda params.conda_env } 5.1.3.2 URI based environment You can have nextflow install a conda packages for specific process. process RUN { conda &quot;bioconda::samtools=1.20&quot; } You can find what packages can be downloaded this way through sequera containers. For an easy example search for bioconda::samtools on the above link. 5.1.3.3 Setting workflow to conda usage When you want to use the specified conda environments in a workflow you must either: Include -with-conda/-use-conda in the nextflow run command or: Better yet add conda.enabled=true to your nextflow.config file 5.2 Script 5.2.1 Variables Within the script nextflow variables are called as ${samples_id}. Bash variables are called as \\${sample_id} 5.2.2 Other languages Other languages can be used within the nextflow script section. For example python: script &quot;&quot;&quot; #!/usr/bin/env python &quot;&quot;&quot; 5.3 Modules The primary main.nf can become quite large by having a lot of processes. To counteract this each process can be stored in a separate main.nf file. The recommendation is to store them in a directory called modules/local within the main workflow directory. Then each process would be within a main.nf file within various directories. A module main.nf would be as so: #!/usr/bin/env nextflow process RUN { / proces contents } Modules are imported as a process as so: /* * Processes */ include { FASTQC_RAW } from &#39;./modules/local/run/main.nf` It is common to have subdirectories within modules/local grouped by tools. For example if you were performing 16S analysis with qiime2 you may have some of the following modules: include { IMPORT } from &#39;./modules/local/qiime2/import_data/main.nf&#39; include { CUTADAPT } from &#39;./modules/local/qiime2/cutadpat/main.nf&#39; include { DADA2 } from &#39;./modules/local/qiime2/dada2/main.nf&#39; "],["05-Workflow.html", "Chapter 6 Workflow", " Chapter 6 Workflow This section includes some extra features related to workflows. A full link of workflow info can be seen in the nextflow docs "],["06-Params_and_config.html", "Chapter 7 Params and config 7.1 Params 7.2 Config", " Chapter 7 Params and config This section includes some extra features related to params and the config file. A full link of config info can be seen in the nextflow docs 7.1 Params Workflow parameters can be set at the top of the primary main.nf file. /* * Params */ params.dir = &quot;.&quot; params.outdir = &quot;./results&quot; params.sample_name = &quot;Sample_1&quot; params.fastqs = &quot;/home/data/project123/raw_fastqs&quot; params.database = &quot;/home/dbs/db123/db123.fastq&quot; params.conda_env = &quot;/home/conda/envs/example_env&quot; params.rscript = &quot;./rscripts/summary.rscript&quot; 7.1.1 Params to channel When using params containing a file in the workflow it is common practice to convert them to a channel. workflow { database_ch = file(params.database) } More info/examples in Creating channels 7.2 Config You can contain a lot of info in a config file. Your primary config file should be in the same directory as your primary main.nf file and should be named nextflow.config. This will cause it to be used when you nextflow run main.nf -resume. Link to nextflow configuration doc 7.2.1 Process directives For Process directives you want to be the default of all processes in your workflow you can include them in your nextflow.config. process { memory = 100.00GB cups = 8 time = 30.d } 7.2.2 Params in config You can include your params in your config file. params { dir = &quot;.&quot; outdir = &quot;./results&quot; sample_name = &quot;Sample_1&quot; fastqs = &quot;/home/data/project123/raw_fastqs&quot; database = &quot;/home/dbs/db123/db123.fastq&quot; conda_env = &quot;/home/conda/envs/example_env&quot; rscript = &quot;./rscripts/summary.rscript&quot; } Ensure you still use params. as a prefix in the various .nf files. 7.2.3 Conda Rather than needing to include -use-conda in your command line you can add the below to your nextflow.config to make your workflow use conda. conda.enabled = true 7.2.4 Container program There are many different container programs. Below are what you can add to the nextflow.config to get the different ones to work. This includes setting the path to the image (if using one) and enabling the specific container program. /// Docker process.container = &#39;nextflow/examples:latest&#39; docker.enabled = true /// Singularity process.container = &#39;/path/to/singularity.img&#39; singularity.enabled = true If using one of them ensure you specify the containers in the variosu processes. Link to container nextflow docs 7.2.5 Config profiles You can have multiple profiles in a nextflow.config file. To use a profile you would run: nextflow run main.nf -profile laptop To make different profiles you could include the below in your nextflow.config. profiles { laptop { process.executor = &#39;local&#39; docker.enabled = true } hpc { process.executor = &#39;slurm&#39; conda.enabled = true process.resourceLimits = [ memory: 750.GB, cpus: 200, time: 30.d ] } } "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
